# medical_ollama.py  ‚Äî  —Ä–∞–±–æ—Ç–∞–µ—Ç —Ç–æ–ª—å–∫–æ —Å Ollama (REST API)
from __future__ import annotations

import os
import re
from pathlib import Path
from typing import Dict, List, TextIO
import fitz
import pandas as pd
from langchain_community.document_loaders import PyPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_core.documents import Document
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS

import ollama  # pip install ollama

# ------------------------- CONFIG -------------------------
OLLAMA_MODEL: str = "hf.co/unsloth/Mistral-Small-3.2-24B-Instruct-2506-GGUF:Q6_K"          # —Å–Ω–∞—á–∞–ª–∞ ¬´ollama pull <name>¬ª
EMBED_MODEL: str = "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"
SERVICES_FILE: Path = Path("docs/services.xlsx")
CHUNK_SIZE: int = 1_000
CHUNK_OVERLAP: int = 200
TOP_K: int = 50
MAX_INPUT_TOK: int = 128_000
SECTION_PATTERN = re.compile(
    r"^(\d+(?:\.\d+)*)\s+([^\n]+)",  # 1.1, 2.3.4, ‚Ä¶ + title
    re.MULTILINE,
)
# ----------------------------------------------------------


class MedicalAssistant:
    def __init__(self) -> None:
        self.embeddings = self._lazy_embedder()
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=CHUNK_SIZE,
            chunk_overlap=CHUNK_OVERLAP,
        )
        self.db = self._load_or_build_index()

    # ---------- EMBEDDER ----------
    def _lazy_embedder(self):
        return HuggingFaceEmbeddings(
            model_name=EMBED_MODEL,
            model_kwargs={"device": "cpu"},  # –º–æ–∂–Ω–æ "cuda", –µ—Å–ª–∏ –µ—Å—Ç—å
            encode_kwargs={"normalize_embeddings": True},
            cache_folder=".embed_cache",
        )

    # ---------- INDEX ----------
    def _load_or_build_index(self) -> FAISS:
        idx_path = SERVICES_FILE.with_suffix(".faiss")
        if idx_path.exists():
            print("üìÅ Loading cached FAISS index ‚Ä¶")
            return FAISS.load_local(
                str(idx_path.parent),
                self.embeddings,
                index_name=idx_path.stem,
                allow_dangerous_deserialization=True,
            )

        print("üîÑ Building index from", SERVICES_FILE)
        df = pd.read_excel(SERVICES_FILE)
        docs = [
            Document(
                page_content=f"–£—Å–ª—É–≥–∞ {row['ID']}: {row['–ù–∞–∑–≤–∞–Ω–∏–µ']}",
                metadata={
                    "source": "services",
                    "id": str(row["ID"]),
                    "name": str(row["–ù–∞–∑–≤–∞–Ω–∏–µ"]),
                },
            )
            for _, row in df.iterrows()
        ]
        db = FAISS.from_documents(docs, self.embeddings)
        db.save_local(str(idx_path.parent), idx_path.stem)
        print("‚úÖ Index saved to disk")
        return db

    # ---------- PDF ----------
    def load_guidelines(self, pdf_path: str) -> Dict[str, str]:
        """
        Reads a Russian clinical guideline PDF and returns a dict:
            {"1.1 –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ": "...", "1.2 –≠—Ç–∏–æ–ª–æ–≥–∏—è": "...", ...}
        Keeps Cyrillic headings intact.
        """
        if not os.path.exists(pdf_path):
            print("‚ùå PDF not found")
            return {}

        doc = fitz.open(pdf_path)
        full_text = "\n".join(page.get_text("text") for page in doc)
        doc.close()

        # Grab diagnosis name from the first page
        title = re.search(
            r"(–°–∞—Ö–∞—Ä–Ω—ã–π –¥–∏–∞–±–µ—Ç.*?|–ì–∏–ø–µ—Ä—Ç–æ–Ω–∏—è.*?|–ê—Å—Ç–º–∞.*?)\n",
            full_text[:2000],
            re.I,
        )
        self.diagnosis_name = (
            title.group(1).strip() if title else "–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ–µ –∑–∞–±–æ–ª–µ–≤–∞–Ω–∏–µ"
        )
        print(f"‚úÖ Diagnosis: {self.diagnosis_name}")

        # Split by headings
        sections = {}
        splits = SECTION_PATTERN.split(full_text)
        # split returns [prefix, num1, title1, body1, num2, title2, body2, ...]
        for i in range(1, len(splits), 3):
            number, title, body = splits[i], splits[i + 1], splits[i + 2]
            key = f"{number} {title}".strip()
            sections[key] = body.strip()

        return sections

    # ---------- UTIL ----------
    @staticmethod
    def _trim_tokens(text: str, max_tok: int) -> str:
        max_chars = int(max_tok * 4.5)
        return (
            text[:max_chars].rsplit("\n", 1)[0]
            if len(text) > max_chars
            else text
        )

    def find_services(self, query: str, k: int = TOP_K) -> List[Document]:
        hits = self.db.similarity_search(query[:300], k=k, fetch_k=k * 3)
        seen, unique = set(), []
        for doc in hits:
            if doc.metadata["id"] not in seen:
                seen.add(doc.metadata["id"])
                unique.append(doc)
        return unique

    # ---------- GENERATE ----------
    def _generate_streaming(self, sections: Dict[str, str], file: TextIO) -> None:
        services = self.find_services(self.diagnosis_name)
        services_list = "\n".join(
            f"- ID {d.metadata['id']}: {d.metadata['name']}" for d in services
        )


        content = sections.get("guidelines", "")
        content = self._trim_tokens(content, MAX_INPUT_TOK)

        system_prompt = (
            "--think=false \n"
            "–¢—ã ‚Äî —Ä—É—Å—Å–∫–æ—è–∑—ã—á–Ω—ã–π –º–µ–¥–∏—Ü–∏–Ω—Å–∫–∏–π –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç. "
            "–¢–≤–æ–π –µ–¥–∏–Ω—Å—Ç–≤–µ–Ω–Ω—ã–π –∏—Å—Ç–æ—á–Ω–∏–∫ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ ‚Äî –∫–ª–∏–Ω–∏—á–µ—Å–∫–∏–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –æ—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è. "
            "–í–∫–ª—é—á–∏ –í–°–ï –¥–∞–Ω–Ω—ã–µ: —á–∏—Å–ª–∞, –µ–¥–∏–Ω–∏—Ü—ã, –ø—Ä–µ–ø–∞—Ä–∞—Ç—ã, –¥–æ–∑—ã, –∫—Ä–∏—Ç–µ—Ä–∏–∏, —Å—Ä–æ–∫–∏, –∏—Å–∫–ª—é—á–µ–Ω–∏—è, –Ω—é–∞–Ω—Å—ã. "
            "–î–ª—è –∫–∞–∂–¥–æ–≥–æ –ø—É–Ω–∫—Ç–∞ —É–∫–∞–∂–∏ —Ç–æ—á–Ω—ã–µ —á–∏—Å–ª–∞, –µ–¥–∏–Ω–∏—Ü—ã, –ø—Ä–µ–ø–∞—Ä–∞—Ç—ã, –¥–æ–∑—ã, —Å—Ä–æ–∫–∏, –∏—Å–∫–ª—é—á–µ–Ω–∏—è."
            "–ï—Å–ª–∏ –≤ —Ä–µ–∫–æ–º–µ–Ω–∞–¥—Ü–∏—è—Ö —è–≤–Ω–æ –Ω–∞–ø–∏—Å–∞–Ω–æ ¬´–Ω–µ –ø—Ä–∏–º–µ–Ω—è—Ç—å¬ª / ¬´–Ω–µ –¥–µ–ª–∞—Ç—å¬ª ‚Äî –≤–∫–ª—é—á–∏ —ç—Ç–æ. "
            " –ï—Å–ª–∏ –≤ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è—Ö –æ–¥–Ω–æ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –∏—Å–∫–ª—é—á–∞–µ—Ç –¥—Ä—É–≥–æ–µ - –≤–∫–ª—é—á–∏ —ç—Ç–æ –∏ —Ä–∞—Å–ø–∏—à–∏. –ù–∞–ø—Ä–∏–º–µ—Ä, —Ç–µ—Å—Ç —Ç–æ–ª–µ—Ä–∞–Ω—Ç–Ω–æ—Å—Ç–∏ –ø—Ä–∏ —Å–∞—Ö–∞—Ä–Ω–æ–º –¥–∏–∞–±–µ—Ç–µ."
            "–ò—Å–ø–æ–ª—å–∑—É–π –º–∞—Ä–∫–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Å–ø–∏—Å–∫–∏ –∏ –ø–æ–¥—Ä–∞–∑–¥–µ–ª—ã. "
            "–í—Å–µ –æ—Ç–≤–µ—Ç—ã —Ñ–æ—Ä–º–∏—Ä—É—é—Ç—Å—è –∏—Å–∫–ª—é—á–∏—Ç–µ–ª—å–Ω–æ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ç–µ–∫—Å—Ç–∞ –∫–ª–∏–Ω–∏—á–µ—Å–∫–∏—Ö —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π "
            "–û—Ç–≤–µ—á–∞–π –ø–æ–¥—Ä–æ–±–Ω–æ c —à–∏—Ä–æ–∫–∏–º–∏ –æ–±—å—è—Å–Ω–µ–Ω–∏—è–º–∏, –º–µ—Ç—Ä–∏–∫–∞–º–∏  –∏ –ø–æ –¥–µ–ª—É, —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä—É—è —Ç–µ–∫—Å—Ç –ø–æ —Ä–∞–∑–¥–µ–ª–∞–º: –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ, –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞, –ª–µ—á–µ–Ω–∏–µ, –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥, –ø—Ä–æ—Ñ–∏–ª–∞–∫—Ç–∏–∫–∞."
        )
        user_prompt = f"""
        –ù–∞ –æ—Å–Ω–æ–≤–∞–Ω–∏–∏ —Ç–µ–∫—Å—Ç–∞ –ö–ª–∏–∏—á–µ—Å–∫–∏—Ö —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π –ø–æ—Å—Ç—Ä–æ–π **–º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ –ø–æ–ª–Ω—ã–π –∏ –¥–µ—Ç–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π** –∞–ª–≥–æ—Ä–∏—Ç–º –ø–æ –∑–∞–±–æ–ª–µ–≤–∞–Ω–∏—é ¬´{self.diagnosis_name}¬ª.

        –¢—Ä–µ–±–æ–≤–∞–Ω–∏—è:
        - –í–∫–ª—é—á–∏ –≤—Å–µ —á–∏—Å–ª–æ–≤—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è, –µ–¥–∏–Ω–∏—Ü—ã –∏–∑–º–µ—Ä–µ–Ω–∏—è, –Ω–∞–∑–≤–∞–Ω–∏—è –ø—Ä–µ–ø–∞—Ä–∞—Ç–æ–≤, –¥–æ–∑—ã, –∫—Ä–∏—Ç–µ—Ä–∏–∏ –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∏, —Å—Ä–æ–∫–∏ –Ω–∞–±–ª—é–¥–µ–Ω–∏—è.
        - –£–∫–∞–∂–∏ **–≤—Å–µ –∏—Å–∫–ª—é—á–µ–Ω–∏—è –∏ –æ—Å–æ–±—ã–µ —Å–ª—É—á–∞–∏**, —É–ø–æ–º—è–Ω—É—Ç—ã–µ –≤ –¥–æ–∫—É–º–µ–Ω—Ç–µ.
        - –í–∫–ª—é—á–∏ –í–°–ï —á–∏—Å–ª–æ–≤—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è, –µ–¥–∏–Ω–∏—Ü—ã, –Ω–∞–∑–≤–∞–Ω–∏—è, –¥–æ–∑—ã, –∫—Ä–∏—Ç–µ—Ä–∏–∏, –∏—Å–∫–ª—é—á–µ–Ω–∏—è –∏ –ø—Ä—è–º–æ –ø—Ä–æ—Ü–∏—Ç–∏—Ä—É–π –∑–∞–ø—Ä–µ—Ç—ã/–æ—Å–æ–±—ã–µ —Å–ª—É—á–∞–∏
        - –°—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä—É–π –ø–æ —Ä–∞–∑–¥–µ–ª–∞–º:

        ### I. –ü—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞
        - –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∑–∞–±–æ–ª–µ–≤–∞–Ω–∏—è
        - –≠–ø–∏–¥–µ–º–∏–æ–ª–æ–≥–∏—è, —Ñ–∞–∫—Ç–æ—Ä—ã —Ä–∏—Å–∫–∞
        - –ö–ª–∏–Ω–∏—á–µ—Å–∫–∏–µ —Å–∏–º–ø—Ç–æ–º—ã –∏ –ø—Ä–∏–∑–Ω–∞–∫–∏

        ### II. –î–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞
        1. –ü–µ—Ä–≤–∏—á–Ω–æ–µ –æ–±—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ (–∂–∞–ª–æ–±—ã, –∞–Ω–∞–º–Ω–µ–∑, –æ—Å–º–æ—Ç—Ä)
        2. –õ–∞–±–æ—Ä–∞—Ç–æ—Ä–Ω—ã–µ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è (–≤—Å–µ –ø–æ–∫–∞–∑–∞—Ç–µ–ª–∏, –Ω–æ—Ä–º—ã, –∏—Å–∫–ª—é—á–µ–Ω–∏—è)
        3. –ò–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞–ª—å–Ω—ã–µ –∏ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –º–µ—Ç–æ–¥—ã
        4. –î–∏—Ñ—Ñ–µ—Ä–µ–Ω—Ü–∏–∞–ª—å–Ω–∞—è –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞ (–ø–æ–ª–Ω—ã–π —Å–ø–∏—Å–æ–∫)
        5. –ö–∞–∫–∏–µ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è –∏—Å–∫–ª—é—á–∞—é—Ç –¥—Ä—É–≥–∏–µ –∏ –≤ –∫–∞–∫–∏—Ö —Å–ª—É—á–∞—è—Ö (–ø–æ–ª–Ω—ã–π —Å–ø–∏—Å–æ–∫ –∏ –ø–æ–¥—Ä–æ–±–Ω–æ –∫–æ–≥–¥–∞ –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è)

        ### III. –õ–µ—á–µ–Ω–∏–µ
        1. –ù–µ–º–µ–¥–∏–∫–∞–º–µ–Ω—Ç–æ–∑–Ω—ã–µ –º–µ—Ä–æ–ø—Ä–∏—è—Ç–∏—è
        2. –ú–µ–¥–∏–∫–∞–º–µ–Ω—Ç–æ–∑–Ω–∞—è —Ç–µ—Ä–∞–ø–∏—è (–ø—Ä–µ–ø–∞—Ä–∞—Ç—ã, –¥–æ–∑—ã, —Ä–µ–∂–∏–º, –∫–æ—Ä—Ä–µ–∫—Ü–∏–∏)
        3. –ü—Ä–æ—Ü–µ–¥—É—Ä—ã / –æ–ø–µ—Ä–∞—Ü–∏–∏ / –≤–º–µ—à–∞—Ç–µ–ª—å—Å—Ç–≤–∞
        4. –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ (—á–∞—Å—Ç–æ—Ç–∞, —Ü–µ–ª–µ–≤—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è)

        ### IV. –†–µ–∞–±–∏–ª–∏—Ç–∞—Ü–∏—è –∏ –¥–æ–ª–≥–æ—Å—Ä–æ—á–Ω–æ–µ –Ω–∞–±–ª—é–¥–µ–Ω–∏–µ
        - –°—Ä–æ–∫–∏, –º–µ—Ç–æ–¥—ã, –∫—Ä–∏—Ç–µ—Ä–∏–∏ –æ—Ç–≤–µ—Ç–∞

        ### V. –û—Å–æ–±—ã–µ —Å–∏—Ç—É–∞—Ü–∏–∏ –∏ –ø—Ä–æ—Ñ–∏–ª–∞–∫—Ç–∏–∫–∞
        - –û—Å–ª–æ–∂–Ω–µ–Ω–∏—è, –ø—Ä–æ—Ç–∏–≤–æ–ø–æ–∫–∞–∑–∞–Ω–∏—è, —ç–∫—Å—Ç—Ä–µ–Ω–Ω—ã–µ —Å–æ—Å—Ç–æ—è–Ω–∏—è
        - –ü—Ä–æ—Ñ–∏–ª–∞–∫—Ç–∏–∫–∞ –∏ –¥–∏—Å–ø–∞–Ω—Å–µ—Ä–∏–∑–∞—Ü–∏—è

        ---

        **–ö–ª–∏–Ω–∏—á–µ—Å–∫–∏–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏:**
        {content}
        """
        messages = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_prompt},
        ]

        try:
            stream = ollama.chat(
                model=OLLAMA_MODEL,
                messages=messages,
                stream=True,
                think=False,
                options={"temperature": 0.6, "top_p": 0.95}

            )
            for chunk in stream:
                token = chunk["message"]["content"]
                file.write(token)
                print(token, end="", flush=True)
        except Exception as e:
            print("\n‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –≤—ã–∑–æ–≤–µ Ollama:", e)

    # ---------- RUN ----------
    def run(self):
        if not self.db:
            print("‚ùå Index not loaded")
            return
        print("ü§ñ Ready. Type PDF path or 'exit'")
        while True:
            pdf = input("üìÑ PDF: ").strip()
            if pdf.lower() in {"exit", "quit", "q"}:
                break
            if not os.path.exists(pdf):
                print("‚ùå File not found")
                continue

            sections = self.load_guidelines(pdf)
            if not sections:
                continue

            outfile = Path("testSec.txt")
            with outfile.open("w", encoding="utf-8") as f:
                for key, text in sections.items():
                    f.write(f"--- {key.upper()} ---\n{text}\n\n")
            print("‚úÖ Raw sections saved ‚Üí", outfile.absolute())
            safe = re.sub(r"[^\w\s-]", "", self.diagnosis_name).strip().replace(" ", "_")[:50]
            outfile = Path(f"test.txt")
            with outfile.open("w", encoding="utf-8") as f:
                f.write(f"# –†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π –∞–ª–≥–æ—Ä–∏—Ç–º –ª–µ—á–µ–Ω–∏—è {self.diagnosis_name}\n\n")
                self._generate_streaming(sections, f)
            print("\n‚úÖ Saved ‚Üí", outfile.absolute())


# ----------------------------------------------------------
if __name__ == "__main__":
    try:
        MedicalAssistant().run()
    except KeyboardInterrupt:
        pass
    except Exception as e:
        print("‚ùå Fatal:", e)